model:
  name: google/gemma-3-1b
  use_flash_attention_2: false

training:
  output_dir: ./outputs/baseline_gemma3_1b
  num_train_epochs: 1
  max_steps: 1000
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: false
  learning_rate: 3.0e-5 # was 2.0e-5
  lr_scheduler_type: cosine
  warmup_ratio: 0.05 # was 0.03
  weight_decay: 0.01
  bf16: true
  tf32: true
  max_grad_norm: 1.0
  logging_steps: 10
  save_strategy: steps
  save_steps: 500
  save_total_limit: 2
  evaluation_strategy: steps
  eval_steps: 200
  load_best_model_at_end: false
  seed: 42
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  remove_unused_columns: false
  padding_label: -100

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none
  task_type: CAUSAL_LM

gating:
  method: baseline

data:
  dataset_name: allenai/tulu-3-sft-mixture
  train_split: "train[:128000]"
  eval_split: "train[128000:132000]"
  max_seq_length: 2048
  prompt_field: prompt
  response_field: response

logging:
  use_wandb: true
  wandb_project: flora
  wandb_run_name: baseline_gemma3_1b
  report_to: wandb
