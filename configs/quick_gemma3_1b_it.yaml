# Quick test config: Gemma-3-1B-it on 256k TÃ¼lu-3 subset

model:
  name: google/gemma-3-1b-it
  use_flash_attention_2: false

training:
  output_dir: ./outputs/quick_gemma3_1b_it
  num_train_epochs: 1
  max_steps: 1000
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  gradient_checkpointing: false
  learning_rate: 2.0e-5
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.01
  bf16: true
  tf32: true
  max_grad_norm: 1.0
  logging_steps: 10
  save_strategy: steps
  save_steps: 500
  save_total_limit: 2
  evaluation_strategy: steps
  eval_steps: 250
  load_best_model_at_end: false
  seed: 42
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  remove_unused_columns: false

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none
  task_type: CAUSAL_LM

slora:
  enable: true
  m: 1024 # dimension of projected space
  k: 32 # rank
  target_accept_rate: 0.40
  initial_threshold: 0.05
  controller_lr: 0.01
  burn_in: 100
  seed: 0
  reorth_every: 32
  k_topk: 128 # top-k logits for sparse sketch
  random: false # if true randomly drop batches to match target_accept_rate
  subspace_decay: 0.99 # exponential decay for old subspace directions (1.0 = no decay)

data:
  dataset_name: allenai/tulu-3-sft-mixture
  train_split: "train[:128000]"
  eval_split: "train[128000:132000]"
  max_seq_length: 512
  prompt_field: prompt
  response_field: response

logging:
  use_wandb: true
  wandb_project: slora
  wandb_run_name: quick_gemma3_1b_it
  report_to: wandb
