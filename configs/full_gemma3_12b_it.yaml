# Full experiment config: Gemma-3-12B on 400k TÃ¼lu-3

model:
  name: google/gemma-3-12b-it # Using gemma-3-12b-it as placeholder
  use_flash_attention_2: true
  load_in_4bit: true
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true

training:
  output_dir: ./outputs/full_gemma3_12b_it
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 32
  gradient_checkpointing: true
  learning_rate: 1.5e-5
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.01
  bf16: true
  tf32: true
  max_grad_norm: 1.0
  logging_steps: 10
  save_strategy: steps
  save_steps: 5000
  save_total_limit: 3
  evaluation_strategy: steps
  eval_steps: 1000
  load_best_model_at_end: false
  seed: 42
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  remove_unused_columns: false

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none
  task_type: CAUSAL_LM

slora:
  enable: true
  m: 512
  k: 64
  tau_n: 0.30
  burn_in: 3000
  seed: 0
  reorth_every: 128

data:
  dataset_name: ai2-tulu/tulu-3-sft-mixture
  train_split: "train[:400000]"
  eval_split: "train[400000:410000]"
  max_seq_length: 4096
  prompt_field: prompt
  response_field: response

logging:
  use_wandb: true
  wandb_project: slora
  wandb_run_name: full_gemma3_12b_it
  report_to: wandb
